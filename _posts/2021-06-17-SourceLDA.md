---
layout: blogpage
title: Source-LDA on Earth
comments: true
tags: Note
---

ä¸ºäº†å‘¨ä¸€çš„åˆ†äº«æ¥å†™ä¸€å†™Source-LDAçš„ç¬”è®°ã€‚

ç¿»è¯‘ä¸€ä¸‹è®ºæ–‡[1]å…ˆï¼

## Abstract ##

Abstractâ€”Topic modeling has increasingly attracted interests
from researchers. Common methods of topic modeling usually
produce a collection of unlabeled topics where each topic is depicted by a distribution of words. Associating semantic meaning with these word distributions is not always straightforward.
Traditionally, this task is left to human interpretation. Manually
labeling the topics is unfortunately not always easy, as topics
generated by unsupervised learning methods do not necessarily
align well with our prior knowledge in the subject domains.

ä¸»é¢˜å»ºæ¨¡è¶Šæ¥è¶Šå¸å¼•ç ”ç©¶è€…çš„å…´è¶£äº†ã€‚ä¸»é¢˜å»ºæ¨¡çš„é€šå¸¸åšæ³•æ˜¯ï¼Œç”Ÿæˆæœªæ ‡è®°ä¸»é¢˜çš„é›†åˆï¼Œå…¶ä¸­æ¯ä¸ªä¸»é¢˜ç”¨å•è¯çš„åˆ†å¸ƒæ¥æè¿°ã€‚è”ç³»è¯­ä¹‰äºè¿™äº›å•è¯åˆ†å¸ƒä¸æ˜¯æ€»æ˜¯éå¸¸ç›´è§‚çš„ã€‚ä¼ ç»Ÿè¯´æ¥ï¼Œè¿™é¡¹å·¥ä½œå°±æ˜¯ç•™ç»™äººå»ç¿»è¯‘çš„ã€‚ä¸å¹¸çš„æ˜¯æ‰‹åŠ¨æ ‡è®°æ ‡é¢˜ä¹Ÿä¸æ€»æ˜¯å®¹æ˜“çš„ï¼Œå› ä¸ºæ— ç›‘ç£å­¦ä¹ æ–¹æ³•ä¸ä¸€å®šä¼šå’Œæˆ‘ä»¬åœ¨å­¦ç§‘é¢†åŸŸçš„å…ˆéªŒçŸ¥è¯†ä¿æŒä¸€è‡´ã€‚


Currently, two approaches to solve this issue exist. The first is a post-processing procedure that assigns each topic with a label from the prior knowledge base that is semantically closest to the word distribution of the topic. 
The second is a supervised topic modeling approach that restricts the topics to a predefined set whose word distributions are provided beforehand. 

Neither approach is ideal, as the former may produce labels that do not
accurately describe the word distributions, and the latter lacks
the ability to detect unknown topics that are crucial to enrich our
knowledge base. 

ç›®å‰å­˜åœ¨æœ‰ä¸¤ç§è§£å†³æ­¤é—®é¢˜çš„æ–¹æ³•ï¼Œç¬¬ä¸€ç§æ˜¯åˆ†é…æ¯ä¸ªè¯é¢˜ç»™ä¸€ä¸ªä»å…ˆéªŒçŸ¥è¯†ä¸­å¾—çŸ¥çš„è¯­ä¹‰ä¸Šæœ€æ¥è¿‘è¯¥è¯é¢˜çš„å•è¯åˆ†å¸ƒçš„æ ‡ç­¾çš„åå¤„ç†è¿‡ç¨‹ã€‚

ç¬¬äºŒä¸ªæ˜¯å°†ä¸»é¢˜é™åˆ¶åœ¨åˆ†å¸ƒå·²ç»å®ç°æä¾›å¥½çš„é¢„å®šä¹‰å•è¯é›†åˆä¸­çš„æœ‰ç›‘ç£ä¸»é¢˜å»ºæ¨¡æ–¹æ³•ã€‚

ä¸¤è€…æ–¹æ³•éƒ½ä¸æ˜¯ç†æƒ³çš„ï¼Œå› ä¸ºå‰è€…å¯èƒ½ä¼šäº§ç”Ÿä¸èƒ½å‡†ç¡®æè¿°è¯åˆ†å¸ƒçš„æ ‡ç­¾ï¼Œåè€…ç¼ºä¹æ£€æµ‹æœªçŸ¥ä¸»é¢˜çš„èƒ½åŠ›ï¼Œè€Œè¿™å¯¹ä¸°å¯Œæˆ‘ä»¬çš„çŸ¥è¯†åº“è‡³å…³é‡è¦ã€‚

Our goal in this paper is to introduce a semisupervised Latent Dirichlet allocation (LDA) model, Source-LDA,
which incorporates prior knowledge to guide the topic modeling process to improve both the quality of the resulting topics and of the topic labeling. 
We accomplish this by integrating existing labeled knowledge sources representing known potential topics into a probabilistic topic model. These knowledge sources are translated into a distribution and used to set the hyperparameters of the Dirichlet generated distribution over words. This approach ensures that the topic inference process is consistent with existing knowledge, and simultaneously, allows for discovery of new topics. The results show improved topic generation and increased accuracy in topic labeling when compared to those obtained using various labeling approaches based off LDA.

æˆ‘ä»¬åœ¨æœ¬æ–‡ä¸­çš„ç›®æ ‡æ˜¯ä»‹ç»ä¸€ä¸ªåŠç›‘ç£çš„éšå«ç‹„åˆ©å…‹é›·åˆ†å¸ƒï¼ˆLDAï¼‰æ¨¡å‹ï¼ŒSource-LDAï¼Œå®ƒç»“åˆäº†å…ˆéªŒçŸ¥è¯†æ¥æŒ‡å¯¼ä¸»é¢˜å»ºæ¨¡è¿‡ç¨‹ä»¥æé«˜ç”Ÿæˆä¸»é¢˜å’Œæ ‡è®°ä¸»é¢˜çš„è´¨é‡ã€‚æˆ‘ä»¬é€šè¿‡<u>å°†ã€è¡¨ç¤ºå·²çŸ¥æ½œåœ¨ä¸»é¢˜çš„ã€‘ã€ç°æœ‰çš„ã€‘ã€æœ‰æ ‡ç­¾çŸ¥è¯†æºã€‘ é›†æˆåˆ°æ¦‚ç‡ä¸»é¢˜æ¨¡å‹ä¸­æ¥å®ç°è¿™ä¸€ç‚¹ã€‚</u>è¿™äº›çŸ¥è¯†æºè¢«è½¬æ¢ä¸ºåˆ†å¸ƒå¹¶ç”¨äºè®¾ç½® Dirichlet ç”Ÿæˆè¯åˆ†å¸ƒçš„è¶…å‚æ•°ã€‚è¿™ç§æ–¹æ³•ç¡®ä¿ä¸»é¢˜æ¨ç†è¿‡ç¨‹ä¸ç°æœ‰çš„çŸ¥è¯†ä¸€è‡´ï¼ŒåŒæ—¶å…è®¸å‘ç°æ–°çš„ä¸»é¢˜ã€‚ç»“æœæ˜¾ç¤ºäº†<u>æ”¹è¿›çš„ä¸»é¢˜ç”Ÿæˆ(topic generation)</u>å’Œ<u>è¯é¢˜æ ‡æ³¨ä¸­(topic labeling)å¢åŠ çš„å‡†ç¡®åº¦</u>ï¼Œä¸ä½¿ç”¨åŸºäº LDA çš„å„ç§æ ‡è®°æ–¹æ³•è·å¾—çš„æ•ˆæœç›¸æ¯”ã€‚

## I. INTRODUCTION ##

Existing topic modeling is often based off Latent Dirichlet allocation (LDA)[1] and involves analyzing a given corpus to produce a distribution over words for each latent topic and a distribution over latent topics for each document. The distributions representing topics are often useful and generally representative of a linguistic topic. Unfortunately, assigning labels to these topics is often left to manual interpretation.

ç°æœ‰çš„ä¸»é¢˜å»ºæ¨¡é€šå¸¸åŸºäºéšå«ç‹„åˆ©å…‹é›·åˆ†å¸ƒ (LDA) å¹¶æ¶‰åŠåˆ†æç»™å®šçš„è¯­æ–™åº“ä¸ºæ¯ä¸ªæ½œåœ¨ä¸»é¢˜ç”Ÿæˆå•è¯åˆ†å¸ƒï¼Œä¸ºæ¯ä¸ªæ–‡æ¡£ç”Ÿæˆæ½œåœ¨ä¸»é¢˜åˆ†å¸ƒã€‚è¿™ä¸ªè¡¨ç¤ºä¸»é¢˜çš„åˆ†å¸ƒé€šå¸¸å¾ˆæœ‰ç”¨ï¼Œè€Œä¸”é€šå¸¸ä»£è¡¨ä¸€ä¸ªè¯­è¨€ä¸Šçš„ä¸»é¢˜ã€‚ä¸å¹¸çš„æ˜¯ï¼Œåˆ†é…è¿™äº›ä¸»é¢˜çš„æ ‡ç­¾é€šå¸¸è¦äººä¸ºè§£é‡Šã€‚

Identifying topic labels is useful in summarizing a set of
words with a single label. For example, words such as pencil, laptop, ruler, eraser, and book can be mapped to the label â€œSchool Supplies.â€ <u>Adding descriptive semantics to each topic can help people, especially those without domain knowledge, to understand topics obtained by topic modeling.</u>

è¯†åˆ«ä¸»é¢˜æ ‡ç­¾æœ‰åŠ©äºæ€»ç»“ä¸€ç»„å¸¦æœ‰å•ä¸ªæ ‡ç­¾çš„å•è¯ã€‚ä¾‹å¦‚ï¼Œåƒé“…ç¬”ã€ç¬”è®°æœ¬ç”µè„‘ã€å°ºå­ã€æ©¡çš®æ“¦å’Œä¹¦æœ¬è¿™æ ·çš„è¯ï¼Œå¯ä»¥æ˜ å°„åˆ°æ ‡ç­¾â€œå­¦æ ¡ç”¨å“ã€‚â€ <u>ä¸ºæ¯ä¸ªä¸»é¢˜æ·»åŠ æè¿°æ€§è¯­ä¹‰å¯ä»¥å¸®åŠ©äººä»¬ï¼Œå°¤å…¶æ˜¯é‚£äº›æ²¡æœ‰é¢†åŸŸçŸ¥è¯†çš„äººï¼Œç†è§£é€šè¿‡ä¸»é¢˜å»ºæ¨¡è·å¾—çš„ä¸»é¢˜ã€‚</u>

A motivating application of accurate topic labeling is to
develop summarization systems for primary care physicians,
who are faced with the challenges of being *inundated* with too much data for a patient and too little time to comprehend it all [2]. The labels can be used to more appropriately and quickly give an overview, or a summary, of patientâ€™s medical history, leading to better outcomes for the patient. This added information can bring significant value to the field of clinical
informatics which already utilizes topic modeling without
labeling [3]â€“[5].

å‡†ç¡®æ ‡è®°ä¸»é¢˜çš„ä¸€ä¸ªæ¿€åŠ±äººçš„åº”ç”¨æ˜¯ä¸ºåˆçº§ä¿å¥åŒ»ç”Ÿå¼€å‘æ€»ç»“ç³»ç»Ÿï¼Œä»–ä»¬é¢ä¸´è¢«æ·¹æ²¡åœ¨æ‚£è€…çš„æ•°æ®å¤ªå¤šï¼Œç†è§£ä»–ä»¬çš„æ—¶é—´å¤ªå°‘çš„æŒ‘æˆ˜ä¸­ã€‚æ ‡ç­¾å¯ç”¨äºæ›´æ°å½“åœ°å’Œå¿«é€Ÿç»™å‡ºæ¦‚è¿°æˆ–æ‚£è€…çš„ç—…å²æ€»ç»“ï¼Œä»è€Œä¸ºæ‚£è€…å¸¦æ¥æ›´å¥½çš„ç»“æœã€‚è¿™å¢åŠ çš„ä¿¡æ¯å¯ä»¥ä¸ºå·²ç»åœ¨åˆ©ç”¨æ— æ ‡ç­¾ä¸»é¢˜å»ºæ¨¡çš„ä¸´åºŠä¿¡æ¯å­¦é¢†åŸŸå¸¦æ¥é‡å¤§ä»·å€¼ã€‚

Existing approaches in labeling topics usually do their
fitting of labels to topics after completion of the unsupervised topic modeling process. A topic produced by this approach may not always match well with any semantic concepts and would therefore be difficult to categorize with a single label.
These problems are best illustrated via a simple case study.

æ ‡è®°ä¸»é¢˜çš„ç°æœ‰æ–¹æ³•é€šå¸¸ä¼šåœ¨å®Œæˆæ— ç›‘ç£ä¸»é¢˜å»ºæ¨¡è¿‡ç¨‹ååšæ ‡ç­¾åˆ°ä¸»é¢˜çš„æ‹Ÿåˆã€‚è¿™ç§æ–¹æ³•äº§ç”Ÿçš„ä¸»é¢˜å¯èƒ½ä¸ä¼šæ€»æ˜¯ä¸è¯­ä¹‰æ¦‚å¿µå¾ˆå¥½åœ°åŒ¹é…ï¼Œå¹¶ä¸”å› ä¼šæ­¤å¾ˆéš¾ç”¨å•ä¸€æ ‡ç­¾è¿›è¡Œåˆ†ç±»ã€‚è¿™äº›é—®é¢˜æœ€å¥½é€šè¿‡ä¸€ä¸ªç®€å•çš„æ¡ˆä¾‹ç ”ç©¶æ¥è¯´æ˜ã€‚


*1) Case Study:* Suppose a corpus of a news source that
consists of two articles is given by documents d1 and d2 each with three words:

*1) æ¡ˆä¾‹ç ”ç©¶ï¼š*å‡è®¾ä¸€ä¸ªæ–°é—»æºçš„è¯­æ–™åŒ…å«ä¸¤ç¯‡æ–‡ç« ï¼Œç”±æ–‡æ¡£ d1 å’Œ d2 ç»™å‡ºï¼Œæ¯ç¯‡å«3ä¸ªå•è¯ã€‚

> d1 - pencil, pencil, umpire
>
> d2 - ruler, ruler, baseball

LDA (with the traditionally used collapsed Gibbs sampler,
standard hyperparameters and the number of topics (K) set
as two) would output different results for different runs due to the inherent stochastic nature. It is very possible to obtain the following result of topic assignments:

å¯èƒ½å¾—åˆ°è¿™æ ·çš„ä¸»é¢˜åˆ†é…ï¼š

> d1 - pencil^1, pencil^1, umpire^2
> 
> d2 - ruler^2, ruler^2, baseball^1

But these assignments to topics differs from the ideal solution that involves knowing the context of the topics in which these words come from. If the topic modeling was to incorporate prior knowledge about the topics â€œSchool Suppliesâ€ and â€œBaseballâ€, then a topic modeling process will more likely generate the ideal topic assignments of:

ä½†è¿™äº›ä¸»é¢˜åˆ†é…å’Œå·²çŸ¥è¿™äº›è¯æ¥æºçš„ä¸»é¢˜ä¸Šä¸‹æ–‡çš„ç†æƒ³æ–¹æ¡ˆæœ‰å·®å¼‚ã€‚å¦‚æœä¸»é¢˜å»ºæ¨¡åŒ…å«æœ‰å…³â€œå­¦æ ¡ç”¨å“â€å’Œâ€œç½‘çƒâ€çš„å…ˆéªŒçŸ¥è¯†ï¼Œé‚£ä¹ˆä¸€ä¸ªä¸»é¢˜å»ºæ¨¡è¿‡ç¨‹æ›´åº”è¯¥ç”Ÿæˆè¿™æ ·çš„ç†æƒ³ä¸»é¢˜åˆ†é…ï¼š

> d1 - pencil^2, pencil^2, umpire^1
>
> d2 - ruler^2, ruler^2, baseball^1

and assign a label of â€œSchool Suppliesâ€ to topic 1 and
â€œBaseballâ€ to topic 2. Furthermore it is advantageous to incorporate this prior knowledge during the topic modeling process.
Consider the following table displaying four different mapping techniques of the first result using the Wikipedia articles of â€œSchool Suppliesâ€ and â€œBaseballâ€ as the prior knowledge:

å¹¶ä¸”åˆ†é…ä¸€ä¸ªâ€œå­¦æ ¡ç”¨å“â€çš„æ ‡ç­¾ç»™ä¸»é¢˜1ï¼Œä¸€ä¸ªâ€œç½‘çƒâ€ç»™ä¸»é¢˜2ã€‚æ­¤å¤–ï¼Œåœ¨ä¸»é¢˜å»ºæ¨¡è¿‡ç¨‹ä¸­åŒ…å«è¿™ä¸ªå…ˆéªŒçŸ¥è¯†æ˜¯æœ‰åˆ©çš„ã€‚
è€ƒè™‘ä¸‹é¢è¡¨æ ¼æ˜¾ç¤ºçš„å››ç§ä¸åŒçš„æ˜ å°„æŠ€æœ¯ï¼Œä½¿ç”¨ç»´åŸºç™¾ç§‘çš„â€œå­¦æ ¡ç”¨å“â€å’Œâ€œç½‘çƒâ€æ–‡ç« ä½œä¸ºå…ˆéªŒçŸ¥è¯†å¾—åˆ°çš„ç¬¬ä¸€ç»“æœï¼š

| Technique | Topic 1 | Topic 2 |
|  ----  | ----  | ---- |
| JS Divergence | Baseball | Baseball | 
| TF-IDF/CS | (same) | (same) | 
| Counting | Baseball | Baseball | 
| PMI | (same)|  (same) | 

Applying this labeling post topic modeling can lead to problems dealing with the topic themselves. This is not so much a problem of the mapping techniques but of the topics used as input. By <u>separating the topics during inference</u> this problem of combining different semantic topics can be avoided.

åº”ç”¨æ­¤æ ‡è®°æ–‡ç« ä¸»é¢˜çš„å»ºæ¨¡æ–¹æ³•å¯èƒ½ä¼šå¯¼è‡´å¤„ç†ä¸»é¢˜æœ¬èº«æ—¶å‡ºç°é—®é¢˜ã€‚è¿™ä¸æ˜¯ä¸€ä¸ªæ˜ å°„æŠ€æœ¯çš„é—®é¢˜ï¼Œæ˜¯ä¸»é¢˜ä½œä¸ºè¾“å…¥çš„é—®é¢˜ã€‚é€šè¿‡åœ¨æ¨ç†æœŸé—´åˆ†ç¦»ä¸»é¢˜ï¼Œå¯ä»¥é¿å…ç»„åˆä¸åŒè¯­ä¹‰ä¸»é¢˜çš„é—®é¢˜ã€‚


To overcome this problem, one may take a supervised
approach that incorporates such prior knowledge into the topic modeling process to improve the quality of topic assignments and more effectively label topics. However, existing supervised approaches [6]â€“[8] are either too lenient or too strict. For example, in the Concept-topic model (CTM) [6], a multinomial distribution is placed over known concepts with associated word sets. This pioneering approach does integrate prior knowledge, but does not take into account word distributions.
For example if a document is generated about the topic â€œSchool Suppliesâ€ it is much more probable to see the word â€œpencilâ€ than the word â€œcompassâ€ even though both words may be associated with the topic â€œSchool Suppliesâ€. This technique also requires some supervision which requires manually inputting preexisting concepts and their bags of words.

ä¸ºäº†å…‹æœè¿™ä¸ªé—®é¢˜ï¼Œå¯ä»¥é‡‡å–ä¸€ä¸ªæœ‰ç›‘ç£çš„å°†æ­¤ç±»å…ˆéªŒçŸ¥è¯†çº³å…¥ä¸»é¢˜å»ºæ¨¡è¿‡ç¨‹çš„æ–¹æ³•æ¥æé«˜ä¸»é¢˜åˆ†é…çš„è´¨é‡å¹¶æ›´æœ‰æ•ˆåœ°æ ‡è®°ä¸»é¢˜ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç›‘ç£æ–¹æ³•è¦ä¹ˆå¤ªå®½æ¾ï¼Œè¦ä¹ˆå¤ªä¸¥æ ¼ã€‚ä¾‹å¦‚ï¼Œåœ¨æ¦‚å¿µä¸»é¢˜æ¨¡å‹ (CTM)ä¸­ï¼Œå¤šé¡¹å¼åˆ†å¸ƒè¢«æ”¾ç½®åœ¨æœ‰å…³è”å•è¯é›†åˆçš„å·²çŸ¥æ¦‚å¿µä¸Šã€‚è¿™ç§å¼€åˆ›æ€§çš„æ–¹æ³•ç¡®å®æ•´åˆäº†å…ˆå‰çš„çŸ¥è¯†ï¼Œä½†ä¸è€ƒè™‘è¯åˆ†å¸ƒã€‚ä¾‹å¦‚ï¼Œå¦‚æœä¸€ç¯‡æ–‡ç« æ˜¯ä»¥ä¸»é¢˜â€œå­¦æ ¡ç”¨å“â€ç”Ÿæˆçš„ï¼Œæ¯”èµ·â€œæŒ‡å—é’ˆâ€è¿™ä¸ªè¯ï¼Œæ›´å¯èƒ½çœ‹åˆ°â€œé“…ç¬”â€è¿™ä¸ªè¯ã€‚å³ä½¿è¿™ä¸¤ä¸ªè¯éƒ½å¯èƒ½æ˜¯â€”â€”ä¸â€œå­¦æ ¡ç”¨å“â€ä¸»é¢˜ç›¸å…³ã€‚è¿™ç§æŠ€æœ¯ä¹Ÿéœ€è¦ä¸€äº›ç›‘ç£ï¼Œéœ€è¦æ‰‹åŠ¨è¾“å…¥é¢„å…ˆå­˜åœ¨çš„æ¦‚å¿µåŠå…¶è¯è¢‹ã€‚

Another approach given by Hansen et al. as explicit
Dirichlet allocation [7] incorporates a preexisting distribution
based off Wikipedia but does not allow for variance from
the Wikipedia distribution. This approach fulfills the goal
of incorporating prior knowledge with their distributions but
requires the topic in the generated corpus to strictly follow the
Wikipedia word distributions.


Hansen ç­‰äººç»™å‡ºçš„å¦ä¸€ç§æ–¹æ³• - è¿™ç§æ–¹æ³•å®ç°äº†ç›®æ ‡å°†å…ˆéªŒçŸ¥è¯†ä¸å…¶åˆ†å¸ƒç›¸ç»“åˆï¼Œä½†è¦æ±‚ç”Ÿæˆçš„è¯­æ–™åº“ä¸­çš„ä¸»é¢˜ä¸¥æ ¼éµå¾ªç»´åŸºç™¾ç§‘è¯åˆ†å¸ƒã€‚

To address these limitations, we propose the Source-LDA
model which is a balance between these two approaches. The
goal is to allow for simultaneous discovery of both known and unknown topics. Given a collection of known topics and their word distributions, Source-LDA is able to identify the subset of these topics that appear in a given corpus. It allows some variance in word distributions to the extent that it optimizes the topic modeling. 
A summary of the contributions of this work are:

ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº† Source-LDA æ¨¡å‹æ˜¯è¿™ä¸¤ç§æ–¹æ³•ä¹‹é—´çš„å¹³è¡¡ã€‚<u>ç›®æ ‡æ˜¯å…è®¸åŒæ—¶å‘ç°å·²çŸ¥å’ŒæœªçŸ¥çš„ä¸»é¢˜ã€‚</u>ç»™å®šä¸€ç»„å·²çŸ¥ä¸»é¢˜åŠå…¶è¯åˆ†å¸ƒï¼ŒSource-LDA èƒ½å¤Ÿè¯†åˆ«å‡ºç°åœ¨ç»™å®šè¯­æ–™åº“ä¸­çš„è¿™äº›ä¸»é¢˜çš„å­é›†ã€‚å®ƒå…è®¸ä¸€äº›è¯åˆ†å¸ƒçš„å·®å¼‚ï¼Œåœ¨å®ƒå¯ä¼˜åŒ–ä¸»é¢˜å»ºæ¨¡çš„èŒƒå›´å†…ã€‚å¯¹æœ¬æ–‡è´¡çŒ®çš„æ€»ç»“å·¥ä½œæ˜¯ï¼š


1) We propose a novel technique to topic modeling
in a semi-supervised fashion that takes into account
preexisting topic distributions.

2) We show how to find the appropriate topics in a
corpus given an input set that contains a subset of
the topics used to generate a corpus.

3) We explain how to make use of prior knowledge
sources. In particular, we show how to use Wikipedia
articles to form word distributions.

4) We introduce an approach that allows for variance
from an input topic to the latent topic discovered
during the topic modeling process.

1) æˆ‘ä»¬æå‡ºäº†ä¸€ç§ä»¥åŠç›‘ç£æ–¹å¼è¿›è¡Œä¸»é¢˜å»ºæ¨¡çš„æ–°æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯è€ƒè™‘äº†é¢„å…ˆå­˜åœ¨çš„ä¸»é¢˜åˆ†å¸ƒã€‚

2) æˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•åœ¨ ç»™å®šåŒ…å«ç”¨äºç”Ÿæˆè¯­æ–™åº“çš„ä¸»é¢˜å­é›† çš„è¾“å…¥é›†ä¸­çš„è¯­æ–™åº“ä¸­ æ‰¾åˆ°åˆé€‚çš„ä¸»é¢˜ã€‚

3) æˆ‘ä»¬è§£é‡Šäº†å¦‚ä½•åˆ©ç”¨å…ˆéªŒçŸ¥è¯†æ¥æºã€‚ ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ç»´åŸºç™¾ç§‘æ–‡ç« æ¥å½¢æˆå•è¯åˆ†å¸ƒã€‚

4) æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å…è®¸å†ä¸»é¢˜å»ºæ¨¡è¿‡ç¨‹ä¸­ ä»è¾“å…¥ä¸»é¢˜åˆ°åˆ°æ½œåœ¨ä¸»é¢˜ä¹‹é—´å·®å¼‚è¢«å‘ç°çš„æ–¹æ³•ã€‚

The rest of this paper is organized as follows: In Section
2, we give a brief introduction to the LDA algorithm and
the Dirichlet distribution. A more detailed description of the
Source-LDA algorithm is presented in Section 3. In Section
4, the algorithm is used and evaluated under various metrics.
Related literature is highlighted in Section 5. Section 6 gives
the conclusions of this paper.

For reproducible research, we make all of our code available online.

> https://github.com/ucla-scai/Source-LDA

## II. PRELIMINARIES ##

*A. Dirichlet Distribution*

The Dirichlet distribution is a distribution over probability
mass functions with a specific number of atoms and is commonly used in Bayesian models. A property of the Dirichlet
that is often used in inference of Bayesian models is conjugacy
to the multinomial distribution. This allows for the posterior
of a random variable with a multinomial likelihood and a
Dirichlet prior to also be a Dirichlet distribution.

The parameters are given as a vector denoted by Î±.
The probability density function for a given probability mass
function (PMF) Î¸ and parameter vector Î± of length J is defined
as:

ã€To be editedã€‘
f(Î¸, Î±) = Î“(PJ
i Î±i)
QJ
i Î“(Î±i)
Y
J
i
Î¸
Î±iâˆ’1
i

A sample from the Dirichlet distribution produces a PMF
that is parameterized by Î±. The choice of a particular set of Î±
values influences the outcome of the generated PMF. If all Î±
values are the same (symmetric parameter), as Î± approaches 0,
the probability will be concentrated on a smaller set of atoms.
As Î± approaches infinity, the PMF will become the uniform
distribution. If all Î±i are natural numbers then each individual
Î±i can be thought of as the â€œvirtualâ€ count for the ith value [9].

*B. Latent Dirichlet Allocation*

Latent Dirichlet Allocation (LDA) is the basis for many
existing probabilistic topic models, and the framework for the
approach presented by this paper. Since we enhance the LDA
model in our proposed approach it is worth giving a brief
overview of the algorithm and model of LDA.
LDA is a hierarchical Bayes model which utilizes Dirichlet
priors to estimate the intractable latent variables of the model.
At a high level, LDA is based on a generative model in
which each word of an input document from a corpus is
chosen by first selecting a topic that corresponds to that word
and then selecting the word from a topic-to-word distribution.
Each topic-to-word distribution and word-to-topic distribution
is drawn from its respective Dirichlet distribution. The formal
definition of the generative algorithm over a corpus is:

1. For each of the K topics Ï†k:
2. Choose Ï†k âˆ¼ Dir(Î²)
3. For each of the D documents d:
4. Choose Nd âˆ¼ Poisson(Î¾)
5. Choose Î¸d âˆ¼ Dir(Î±)
6. For each of the Nd words wn,d:
7. Choose zn,d âˆ¼ Multinomial(Î¸)
8. Choose wn,d âˆ¼ Multinomial(Ï†zn,d )

From the generative algorithm the resultant Bayes model
is shown by Figure 1(a).
Bayesâ€™ law is used to infer the latent Î¸ distribution, Ï†
distribution, and z
P(Î¸, Ï†, z|w, Î±, Î²) = p(Î¸, Ï†, z, w|Î±, Î²)
p(w|Î±, Î²)
Unfortunately the exact computation of this equation is intractable. Hence, it must be approximated with techniques such
as expectation-maximization [1], Gibbs sampling or collapsed
Gibbs sampling [10].


## III. PROPOSED APPROACH ##

Source-LDA is an extension of the LDA generative model.
In Source-LDA, after a known set of topics are determined, an initial word-to-topic distribution is generated from corresponding Wikipedia articles. The desiderata is to enhance existing
LDA topic modeling by integrating prior knowledge into the
topic modeling process. The relevant terms and concepts used
in the following discussion are defined below.

Source-LDA æ˜¯ LDA ç”Ÿæˆæ¨¡å‹çš„æ‰©å±•ã€‚åœ¨ Source-LDA ä¸­ï¼Œåœ¨ç¡®å®šäº†ä¸€ç»„å·²çŸ¥çš„ä¸»é¢˜é›†åˆåï¼Œä¸€ä¸ªåˆå§‹çš„ å•è¯-ä¸»é¢˜ åˆ†å¸ƒä»å¯¹åº”ç»´åŸºç™¾ç§‘æ–‡ç« ä¸­ç”Ÿæˆã€‚<u>éœ€è¦çš„æ˜¯é€šè¿‡å°†å…ˆéªŒçŸ¥è¯†æ•´åˆåˆ°ä¸»é¢˜å»ºæ¨¡çš„è¿‡ç¨‹ä¸­æ¥æé«˜ç°æœ‰çš„ LDA ä¸»é¢˜æ¨¡å‹ã€‚</u>ä½¿ç”¨çš„ç›¸å…³æœ¯è¯­å’Œæ¦‚å¿µåœ¨ä¸‹é¢çš„è®¨è®ºä¸­å®šä¹‰å¦‚ä¸‹ã€‚

Definition 1 (Knowledge source): A knowledge source is
a collection of documents that are focused on describing a set
of concepts. For example the knowledge source used in our
experiments are Wikipedia articles that describe the categories
we select from the Reuters dataset.

Definition 2 (Source Distribution): <u>The source distribution is a discrete probability distribution over the words of a document describing a topic. </u> The probability mass function
is given by

![F2](/images/illustration/2021-06-21/F2.png)

where W is the set of all words in the document, G = |W|, and
nwi
is the number of times word wi appears in the document.

Definition 3 (Source Hyperparameters): For a given document in a knowledge source the knowledge source hyperparameters are defined by the vector (X1, X2, . . . , XV ) where
Xi = nwi + and  is a very small positive number that allows
for non-zero probability draws from the Dirichlet distribution.
V is the size of the vocabulary of the corpus for which we are
topic modeling, and nwi
is the number of times the word wi
from the corpus vocabulary appears in the knowledge source
document.


We detail three approaches to capture the intent of SourceLDA. The first approach is a simple enhancement to the LDA
model that allows for the influencing of topic distributions,
but suffers from needing more user intervention. The second
approach allows for the mixing of unknown topics, and the
third approach combines the previous two approaches. It moves
toward a complete solution to topic modeling based off prior
knowledge sources.



C. Source-LDA

By using the counts as hyperparameters, the resultant Ï†
distribution will take on the shape of the word distribution
derived from the knowledge source. However, this might be at odds with the aim of enhancing existing topic modeling. With the goal to influence the Ï† distribution, it is entirely plausible to
have divergence between the two distributions. In other words,
Ï† may not need to strictly follow the corresponding knowledge
source distribution.

é€šè¿‡ä½¿ç”¨è®¡æ•°ä½œä¸ºè¶…å‚æ•°ï¼Œå¾—åˆ°çš„ Ï† åˆ†å¸ƒå°†æœ‰æ¥æºäºçŸ¥è¯†æºçš„å•è¯åˆ†å¸ƒçš„å½¢çŠ¶ã€‚ ç„¶è€Œï¼Œè¿™å¯èƒ½äºä»¥å¢å¼ºç°æœ‰ä¸»é¢˜å»ºæ¨¡çš„ç›®æ ‡ä¸ä¸€è‡´ã€‚
ä¸ºäº†å½±å“ Ï† åˆ†å¸ƒï¼Œä¸¤ä¸ªåˆ†å¸ƒä¹‹é—´å®Œå…¨æœ‰å¯èƒ½å­˜åœ¨å·®å¼‚ã€‚ <u>æ¢å¥è¯è¯´ï¼ŒÏ† å¯èƒ½ä¸éœ€è¦ä¸¥æ ¼éµå¾ªç›¸åº”çš„çŸ¥è¯†æºçš„åˆ†å¸ƒã€‚</u>

1) Variance from the source distribution: To allow for this relaxation, another parameter Î» is introduced into the model which is used to allow for a higher deviance from the source distribution. To obtain this variance each source hyperparameter will be raised to a power of Î». Thus as Î» approaches 0 each hyperparameter will approach 1 and the subsequent Dirichlet draw will allow all discrete distributions with equal probability. As Î» approaches 1 the Dirichlet draw will be tightly conformed to the source distribution.

1) ä¸æºåˆ†å¸ƒçš„æ–¹å·®ï¼šä¸ºäº†å…è®¸è¿™ç§æ¾å¼›ï¼Œå°†å¦ä¸€ä¸ªå‚æ•° Î» å¼•å…¥æ¨¡å‹ä¸­ï¼Œç”¨äºå…è®¸ä¸æºåˆ†å¸ƒçš„æ›´é«˜åå·®ã€‚ ä¸ºäº†è·å¾—è¿™ä¸ªæ–¹å·®ï¼Œæ¯ä¸ªæºè¶…å‚æ•°å°†è¢«æå‡åˆ° Î» çš„å¹‚ã€‚ å› æ­¤ï¼Œå½“ Î» æ¥è¿‘ 0 æ—¶ï¼Œæ¯ä¸ªè¶…å‚æ•°å°†æ¥è¿‘ 1ï¼Œéšåçš„ç‹„åˆ©å…‹é›·ç»˜åˆ¶å°†å…è®¸æ‰€æœ‰ç¦»æ•£åˆ†å¸ƒå…·æœ‰ç›¸ç­‰çš„æ¦‚ç‡ã€‚ å½“ Î» æ¥è¿‘ 1 æ—¶ï¼Œç‹„åˆ©å…‹é›·å›¾å°†ä¸æºåˆ†å¸ƒç´§å¯†ä¸€è‡´ã€‚

The addition of Î» changes the existing generative model
only slightly and allows for a variance for each individual Î´i, which frees us from an overly restrictive binding to the associated knowledge source distribution. The Î» parameter acts as a measure of how much divergence is allowed for a given modeled topic from the knowledge source distribution. Figure 3 shows how the JS Divergence changes with changes to the Î» parameter.

![Figure 3](/images/illustration/2021-06-21/3.png)

æ·»åŠ  Î» åªä¼šç•¥å¾®æ”¹å˜ç°æœ‰çš„ç”Ÿæˆæ¨¡å‹ï¼Œå¹¶å…è®¸æ¯ä¸ªä¸ªä½“ Î´i å­˜åœ¨å·®å¼‚ï¼Œè¿™ä½¿æˆ‘ä»¬æ‘†è„±äº†å¯¹ç›¸å…³çŸ¥è¯†æºåˆ†å¸ƒçš„è¿‡åº¦é™åˆ¶ã€‚<u> Î» å‚æ•°ç”¨ä½œè¡¡é‡ç»™å®šå»ºæ¨¡ä¸»é¢˜ä¸çŸ¥è¯†æºåˆ†å¸ƒçš„å·®å¼‚ç¨‹åº¦çš„åº¦é‡ã€‚</u> å›¾ 3 æ˜¾ç¤ºäº† JS Divergence å¦‚ä½•éšç€ Î» å‚æ•°çš„å˜åŒ–è€Œå˜åŒ–ã€‚

With the introduction of Î» as an input parameter, the new topic model has the advantage of allowing variance and also leaves the collapsed Gibbs sampling equation unchanged. However this also requires a uniform variance from the knowledge base distribution for all latent topics. This can be a problem if the corpus was generated with some topics influenced strongly while others less so. To solve this we can introduce Î» as a hidden parameter of the model.

é€šè¿‡å¼•å…¥ Î» ä½œä¸ºè¾“å…¥å‚æ•°ï¼Œæ–°çš„ä¸»é¢˜æ¨¡å‹å…·æœ‰å…è®¸æ–¹å·®çš„ä¼˜ç‚¹ï¼Œå¹¶ä¸”è¿˜ä¿æŒæŠ˜å çš„ Gibbs é‡‡æ ·æ–¹ç¨‹ä¸å˜ã€‚ ç„¶è€Œï¼Œè¿™ä¹Ÿéœ€è¦æ‰€æœ‰æ½œåœ¨ä¸»é¢˜çš„çŸ¥è¯†åº“åˆ†å¸ƒçš„ç»Ÿä¸€æ–¹å·®ã€‚ å¦‚æœç”Ÿæˆçš„è¯­æ–™åº“ä¸­æŸäº›ä¸»é¢˜å—åˆ°å¼ºçƒˆå½±å“è€Œå…¶ä»–ä¸»é¢˜å½±å“è¾ƒå°ï¼Œåˆ™è¿™å¯èƒ½æ˜¯ä¸€ä¸ªé—®é¢˜ã€‚ ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥å¼•å…¥ Î» ä½œä¸ºæ¨¡å‹çš„éšè—å‚æ•°ã€‚

2) Approximating Î»: In the ideal situation Î» will be as close to 1 for most knowledge based latent topics, with the flexibility to deviate as required by the data. For this we assume a Gaussian prior over Î» with mean set to Âµ. The variance then becomes a modeled parameter that conceptually can be thought of as how much variance from the knowledge source distribution we wish to allow in our topic model. In assuming a Gaussian prior for Î», we must integrate Î» out of the collapsed Gibbs sampling equations (only the probability of wi under topic j is shown, the probability of topic j in document d is unchanged and omitted).

2) è¿‘ä¼¼ Î»ï¼šåœ¨ç†æƒ³æƒ…å†µä¸‹ï¼Œå¯¹äºå¤§å¤šæ•°åŸºäºçŸ¥è¯†çš„æ½œåœ¨ä¸»é¢˜ï¼ŒÎ» å°†æ¥è¿‘ 1ï¼Œå¹¶ä¸”å¯ä»¥æ ¹æ®æ•°æ®çš„è¦æ±‚çµæ´»åœ°åç¦»ã€‚ ä¸ºæ­¤ï¼Œæˆ‘ä»¬å‡è®¾ Î» ä¸Šçš„é«˜æ–¯å…ˆéªŒï¼Œå‡å€¼è®¾ç½®ä¸º Âµã€‚ ç„¶åæ–¹å·®æˆä¸ºä¸€ä¸ªå»ºæ¨¡å‚æ•°ï¼Œä»æ¦‚å¿µä¸Šå¯ä»¥å°†å…¶è§†ä¸ºæˆ‘ä»¬å¸Œæœ›åœ¨æˆ‘ä»¬çš„ä¸»é¢˜æ¨¡å‹ä¸­å…è®¸çš„çŸ¥è¯†æºåˆ†å¸ƒçš„å¤šå°‘æ–¹å·®ã€‚ åœ¨å‡è®¾ Î» ä¸ºé«˜æ–¯å…ˆéªŒæ—¶ï¼Œæˆ‘ä»¬å¿…é¡»å°† Î» ä»æŠ˜å çš„ Gibbs é‡‡æ ·æ–¹ç¨‹ä¸­ç§¯åˆ†å‡ºæ¥ï¼ˆä»…æ˜¾ç¤ºä¸»é¢˜ j ä¸‹ wi çš„æ¦‚ç‡ï¼Œæ–‡æ¡£ d ä¸­ä¸»é¢˜ j çš„æ¦‚ç‡ä¸å˜å¹¶çœç•¥ï¼‰ã€‚

![Formula 4](/images/illustration/2021-06-21/F4.png)


Unfortunately closed form expressions for these integrals are hard to obtain and so they must be approximated numerically during sampling.

ä¸å¹¸çš„æ˜¯ï¼Œè¿™äº›ç§¯åˆ†çš„é—­å¼è¡¨è¾¾å¼å¾ˆéš¾è·å¾—ï¼Œå› æ­¤å¿…é¡»åœ¨é‡‡æ ·æœŸé—´è¿›è¡Œæ•°å€¼è¿‘ä¼¼ã€‚

Another problem arises in that the change of Î» is not in par with the change of the Gaussian distribution, as can be seen in Figure 3. To make the changes of Î» more in line with that expected from the Gaussian PDF, we must map each individual Î» value in the range 0 to 1 with a value which produces a change in the JS divergence in a linear fashion. We approximate a function, g(x) with a linear derivative, shown in Figure 4. The approach taken to approximate g(x) is by linear interpolation of an aggregated large number of samples for each point taken in the range 0 to 1. Our collapsed Gibbs sampling equations then becomes:

å¦ä¸€ä¸ªé—®é¢˜æ˜¯ Î» çš„å˜åŒ–ä¸é«˜æ–¯åˆ†å¸ƒçš„å˜åŒ–ä¸ä¸€è‡´ï¼Œå¦‚å›¾ 3 æ‰€ç¤ºã€‚ ä¸ºäº†ä½¿ Î» çš„å˜åŒ–æ›´ç¬¦åˆé«˜æ–¯ PDF çš„é¢„æœŸï¼Œæˆ‘ä»¬å¿…é¡»æ˜ å°„æ¯ä¸ª åœ¨ 0 åˆ° 1 èŒƒå›´å†…çš„å•ä¸ª Î» å€¼ï¼Œå…¶å€¼ä»¥çº¿æ€§æ–¹å¼äº§ç”Ÿ JS æ•£åº¦çš„å˜åŒ–ã€‚ æˆ‘ä»¬ä½¿ç”¨çº¿æ€§å¯¼æ•°é€¼è¿‘å‡½æ•° g(x)ï¼Œå¦‚å›¾ 4 æ‰€ç¤ºã€‚ é€¼è¿‘ g(x) çš„æ–¹æ³•æ˜¯å¯¹ 0 åˆ° 1 èŒƒå›´å†…çš„æ¯ä¸ªç‚¹çš„èšåˆå¤§é‡æ ·æœ¬è¿›è¡Œçº¿æ€§æ’å€¼ã€‚ æˆ‘ä»¬æŠ˜å çš„ Gibbs é‡‡æ ·æ–¹ç¨‹å˜ä¸ºï¼š

![Formula 5](/images/illustration/2021-06-21/F5.png)

*3) Superset Topic Reduction:* A third problem involves knowing the right mixture of known topics and unknown topics. It is also entirely possible that many known topics may not be used by the generative model. Our desire to leave the model as unsupervised as possible calls for input that is a superset of the actual generative topic selection in order to avoid manual topic selection. In the case of modeling only a specific number of topics over the corpus, the problem then becomes how to choose which knowledge source latent topics to allow in the model vs. how many unlabeled topics to allow.

*3ï¼‰è¶…é›†ä¸»é¢˜å‡å°‘*ï¼šç¬¬ä¸‰ä¸ªé—®é¢˜æ¶‰åŠçŸ¥é“å·²çŸ¥ä¸»é¢˜å’ŒæœªçŸ¥ä¸»é¢˜çš„æ­£ç¡®æ··åˆã€‚ ç”Ÿæˆæ¨¡å‹å¯èƒ½ä¸ä½¿ç”¨è®¸å¤šå·²çŸ¥ä¸»é¢˜ä¹Ÿæ˜¯å®Œå…¨æœ‰å¯èƒ½çš„ã€‚ æˆ‘ä»¬å¸Œæœ›è®©æ¨¡å‹å°½å¯èƒ½ä¸å—ç›‘ç£ï¼Œè¿™è¦æ±‚è¾“å…¥æ˜¯å®é™…ç”Ÿæˆä¸»é¢˜é€‰æ‹©çš„è¶…é›†ï¼Œä»¥é¿å…æ‰‹åŠ¨é€‰æ‹©ä¸»é¢˜ã€‚ åœ¨ä»…å¯¹è¯­æ–™åº“ä¸­ç‰¹å®šæ•°é‡çš„ä¸»é¢˜è¿›è¡Œå»ºæ¨¡çš„æƒ…å†µä¸‹ï¼Œé—®é¢˜å°±å˜æˆäº†å¦‚ä½•é€‰æ‹©æ¨¡å‹ä¸­å…è®¸çš„çŸ¥è¯†æºæ½œåœ¨ä¸»é¢˜ä¸å…è®¸çš„æœªæ ‡è®°ä¸»é¢˜çš„æ•°é‡ã€‚

The goal then is to allow for a superset of knowledge source topics as input and then during the inference to select the best subset of these with a mixture of unknown topics where the total number of unlabeled topics is given as input K. The approach given is to use a mixture of K unlabeled topics alongside the labeled knowledge source topics. The total number of topics then becomes T. During the inference we eliminate topics which are not assigned to any documents. At the end of the sampling phase we then can use a clustering algorithm (such as k-means, JS divergence) to further reduce the modeled topics and give a total of K topics. As described more in the experimental section, with the goal of capturing topics that were frequently occurring in the corpus, topics not appearing in a frequent enough of documents were eliminated.

ç›®æ ‡æ˜¯å…è®¸çŸ¥è¯†æºä¸»é¢˜çš„è¶…é›†ä½œä¸ºè¾“å…¥ï¼Œç„¶ååœ¨æ¨ç†è¿‡ç¨‹ä¸­é€‰æ‹©è¿™äº›æœ€ä½³å­é›†ä¸æœªçŸ¥ä¸»é¢˜çš„æ··åˆï¼Œå…¶ä¸­æœªæ ‡è®°ä¸»é¢˜çš„æ€»æ•°ä½œä¸ºè¾“å…¥ Kã€‚ç»™å‡ºçš„æ–¹æ³•æ˜¯ å°† K ä¸ªæœªæ ‡è®°çš„ä¸»é¢˜ä¸æ ‡è®°çš„çŸ¥è¯†æºä¸»é¢˜æ··åˆä½¿ç”¨ã€‚ ç„¶åä¸»é¢˜çš„æ€»æ•°å˜ä¸º Tã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬æ¶ˆé™¤äº†æœªåˆ†é…ç»™ä»»ä½•æ–‡æ¡£çš„ä¸»é¢˜ã€‚ åœ¨é‡‡æ ·é˜¶æ®µç»“æŸæ—¶ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨èšç±»ç®—æ³•ï¼ˆä¾‹å¦‚ k-meansã€JS divergenceï¼‰è¿›ä¸€æ­¥å‡å°‘å»ºæ¨¡ä¸»é¢˜å¹¶ç»™å‡ºæ€»å…± K ä¸ªä¸»é¢˜ã€‚ å¦‚å®éªŒéƒ¨åˆ†æ‰€è¿°ï¼Œä¸ºäº†æ•è·è¯­æ–™åº“ä¸­é¢‘ç¹å‡ºç°çš„ä¸»é¢˜ï¼Œæ¶ˆé™¤äº†æ²¡æœ‰å‡ºç°åœ¨è¶³å¤Ÿé¢‘ç¹çš„æ–‡æ¡£ä¸­çš„ä¸»é¢˜ã€‚


5) Input determination: Determining the necessary arameters and inputs into LDA is an established research area [21], but since the proposed model introduces additional input requirements a brief overview will be given about how to best set the parameters and determine the knowledge source.

a) Parameter selection: To determine the appropriate
parameters, techniques utilizing log likelihood have previously been established [10]. Since these approaches generally require held out data and are a function of the Ï†, Î¸, and Î± variables the introduction of Î» and Ïƒ will not differentiate from their original equations. For example the perplexity calculations used for Source-LDA are based off of importance sampling [22], or latent variable estimation via Gibbs sampling [23]. Importance sampling is only a function of Ï† given by Equation 4, and estimation via Gibbs sampling can made using Equation 4 and by the following equation (zËœ, wËœ, and nËœ represent the corresponding variables in the test document set):

a) å‚æ•°é€‰æ‹©ï¼šä¸ºäº†ç¡®å®šåˆé€‚çš„å‚æ•°ï¼Œä¹‹å‰å·²ç»å»ºç«‹äº†åˆ©ç”¨å¯¹æ•°ä¼¼ç„¶çš„æŠ€æœ¯[10]ã€‚ ç”±äºè¿™äº›æ–¹æ³•é€šå¸¸éœ€è¦ä¿ç•™æ•°æ®å¹¶ä¸”æ˜¯ Ï†ã€Î¸ å’Œ Î± å˜é‡çš„å‡½æ•°ï¼Œå› æ­¤å¼•å…¥ Î» å’Œ Ïƒ ä¸ä¼šä¸å®ƒä»¬çš„åŸå§‹æ–¹ç¨‹åŒºåˆ†å¼€æ¥ã€‚ ä¾‹å¦‚ï¼Œç”¨äº Source-LDA çš„å›°æƒ‘åº¦è®¡ç®—åŸºäºé‡è¦æ€§é‡‡æ · [22]ï¼Œæˆ–é€šè¿‡ Gibbs é‡‡æ · [23] çš„æ½œåœ¨å˜é‡ä¼°è®¡ã€‚ é‡è¦æ€§æŠ½æ ·åªæ˜¯æ–¹ç¨‹ 4 ç»™å‡ºçš„ Ï† çš„å‡½æ•°ï¼Œå¯ä»¥ä½¿ç”¨æ–¹ç¨‹ 4 å’Œä»¥ä¸‹æ–¹ç¨‹ï¼ˆz~ã€w~å’Œ n~ è¡¨ç¤ºæµ‹è¯•æ–‡æ¡£é›†ä¸­çš„ç›¸åº”å˜é‡ï¼‰é€šè¿‡ Gibbs é‡‡æ ·è¿›è¡Œä¼°è®¡ï¼š



b) Knowledge source selection: Source-LDA is designed to be used only with a corpus which has a known super set of topics which comprise a large portion of the tokens. An example of such a case is that of a corpus consisting of clinical patient notes. Since there are extensive knowledge sources comprising essentially all medical topics, Source-LDA can be useful in discovering and labeling these existing topics. In cases where it is not so easy to collect a superset of topics traditional approaches may be more useful.

çŸ¥è¯†æºé€‰æ‹©ï¼šSource-LDA è®¾è®¡ä¸ºä»…ç”¨äºå…·æœ‰å·²çŸ¥è¶…ä¸»é¢˜é›†çš„è¯­æ–™åº“ï¼Œå…¶ä¸­åŒ…å«å¤§éƒ¨åˆ†æ ‡è®°ã€‚ è¿™ç§æƒ…å†µçš„ä¸€ä¸ªä¾‹å­æ˜¯ç”±ä¸´åºŠæ‚£è€…ç¬”è®°ç»„æˆçš„è¯­æ–™åº“ã€‚ ç”±äºæœ‰å¹¿æ³›çš„çŸ¥è¯†æ¥æºï¼ŒåŸºæœ¬ä¸ŠåŒ…æ‹¬æ‰€æœ‰åŒ»å­¦ä¸»é¢˜ï¼ŒSource-LDA å¯ç”¨äºå‘ç°å’Œæ ‡è®°è¿™äº›ç°æœ‰ä¸»é¢˜ã€‚ åœ¨æ”¶é›†ä¸»é¢˜è¶…é›†ä¸æ˜¯é‚£ä¹ˆå®¹æ˜“çš„æƒ…å†µä¸‹ï¼Œä¼ ç»Ÿæ–¹æ³•å¯èƒ½æ›´æœ‰ç”¨ã€‚


## LDA & pLSA ##

We generalize PLSA by changing the fixed dd to a Dirichlet prior.

The generative process for each word w_jw 
j
â€‹
  (from a vocab of size VV) in document d_id 
i
â€‹
  is as follow:

  


## Reference - This post ##

[1] Wood, Justin, et al. "Source-LDA: Enhancing probabilistic topic models using prior knowledge sources." 2017 IEEE 33rd International Conference on Data Engineering (ICDE). IEEE, 2017. 